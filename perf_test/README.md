# Performance testing

Чтобы провести нагрузочное тестирование, нужно решить несколько вопросов:

## Выбор основных данных для тестирования

В movie-gate есть фильмы, рецензии, профили, персоны, коллекции - автоген или пользовательские. 
В приложении большая часть использования идет на поток чтения.
Нас же интересует ядро проекта - фильмы и автоген коллекции, например популярное. Фильмы также тянут и персон в API.

## Выборка данных

Для тестирования неважно содержание данных, то есть чтобы они были реальными. Условно есть какое-то описание 
и неважно будет ли там что-то осмысленное или набор символов, главное чтобы порядок длинны сообщения был сопоставим 
с реальными данными.

## Тип нагрузки

Множество запросов с малой выборкой - OLTP.

## Инструментарий

- кастомный генератор данных - нужно сгенерировать 1 000 000 записей по фильмам.
- наполнитель данных - нужно создать инструмент, который будет пакетами заливать все необходимые данные.
- специальный [инструмент](https://github.com/tsenart/vegeta) - для нагрузки HTTP сервера с постоянной частотой запросов и документации.

## До оптимизации

DDL можно наблюдать [тут](ddl/begin).

### Результаты тестирования

![image](https://user-images.githubusercontent.com/88785411/212724237-f1f7b2e5-d25c-46ca-b6f1-54ff3c8853d5.png)

По результатам нескольких запусков тестов RPS в диапазоне 2400-2500, Read flow 400-420 KB/s.

## Рекомендации для оптимизации

Провести денормализацию для жанров, компаний, стран, картинок, тегов фильма, то есть ввести дополнительные поля в 
таблице фильмов - массивы строк. Данные статичны, прирост в RPS будет ощутим. В запросе к фильму можно будет избавить от 
5 подзапросов за его частями.

## Проблемы 

Первая проблема появилась когда речь пошла о больших данных. PostgreSQL не тянет более 65535 value для вставки.
Помогла множественная адаптивная пакетная вставка. 
[Пример](../internal/pkg/dev/fillerdb/film.go).

Также нужно было оптимизировать запросы. Нельзя было просто так взять и обновить все денормализованные поля 1 запросом.
То есть будет очень неэффективно делать подзапросы с GROUP BY *_id для всех фильмов. 
Запрос серфил по таблицам с 1 000 000 и 5 500 000 записями. Была попытка такого подхода, запрос 
выполнялся 15 ч до того как офнул.

Но даже с этими решениями полный автоген занимал порядка 38 минут (профит с прошлым вариантом по меньшей мере в 22 раза).
Решение было простым 1 раз полностью сгенерить базу, сделать физичиский backup и подгружать при необходимости. 
Только нужно контролировать [доступ](../scripts/migrations/up/5_user_access.up.sql) ролей отдельно.
Физическое копирование не несет в себе данных о ролях, в отличие от логического. По итогу 38 минут -> 47 сек

Дополнительно стоит отметить, что нужно поднимать таймауты для таких больших запросов. На стороне приложения и на стороне БД.

## Примечание

Конфиг [наполнения](../cmd/filldb/configs/debug.toml).

Конфиг [тестирования](./base.toml).

Конфиг [базы](../configs/servers/default.conf).

API документация [тут](https://app.swaggerhub.com/apis/BugOverload/API-Kino/1.0.0).

Работа наполнителя:

![image](https://user-images.githubusercontent.com/88785411/212561311-1bdc37d9-8f90-44bc-b029-91b7433f2966.png)

334,52s user 132,25s system 19% cpu 38:56,87 total

Работа тестирования:

![image](https://user-images.githubusercontent.com/88785411/212724155-02155e03-4d15-492a-a3be-9f0d64d4b8a0.png)
